<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>AlexNet论文精读 - WilliamLeung的个人博客</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#f7f7f7"><meta name="application-name" content="WilliamLeung的博客"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="msapplication-TileColor" content="#f7f7f7"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="WilliamLeung的博客"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="概述AlexNet是2012年ImageNet竞赛冠军获得者Alex Krizhevsky、Hinton（获2019图灵奖）和Ilya Sutskever（Alpha GO、TensorFlow的开发者）设计的。原论文名为ImageNet Classification with Deep Convolutional Neural Networks（使用深度神经网络对ImageNet进行图像分类），"><meta property="og:type" content="blog"><meta property="og:title" content="AlexNet论文精读"><meta property="og:url" content="http://williamleung.cn/posts/12577bd1/"><meta property="og:site_name" content="WilliamLeung的个人博客"><meta property="og:description" content="概述AlexNet是2012年ImageNet竞赛冠军获得者Alex Krizhevsky、Hinton（获2019图灵奖）和Ilya Sutskever（Alpha GO、TensorFlow的开发者）设计的。原论文名为ImageNet Classification with Deep Convolutional Neural Networks（使用深度神经网络对ImageNet进行图像分类），"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201124162846687.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201124165216562.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/2020112416412246.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/2020112417105897.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201124200434382.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201124202040999.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201124202948287.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201124204109920.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201124205935514.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201124210856807.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201124211256282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201125073041214.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201125074437311.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201124211917693.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201125075506464.png#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201125091952691.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201125092540323.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201126081429194.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201126082104795.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201126083025925.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201126083200580.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201126090300131.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/2020112609024480.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201126090711770.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201126091309315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/2020112611002659.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201126125530400.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="og:image" content="https://img-blog.csdnimg.cn/20201126192043286.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><meta property="article:published_time" content="2021-09-22T19:21:43.000Z"><meta property="article:modified_time" content="2022-05-27T14:23:49.003Z"><meta property="article:author" content="William"><meta property="article:tag" content="AlexNet"><meta property="article:tag" content="论文"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="神经网络"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://img-blog.csdnimg.cn/20201124162846687.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://williamleung.cn/posts/12577bd1/"},"headline":"AlexNet论文精读","image":[],"datePublished":"2021-09-22T19:21:43.000Z","dateModified":"2022-05-27T14:23:49.003Z","author":{"@type":"Person","name":"William"},"publisher":{"@type":"Organization","name":"WilliamLeung的个人博客","logo":{"@type":"ImageObject","url":"http://williamleung.cn/img/logo.svg"}},"description":"概述AlexNet是2012年ImageNet竞赛冠军获得者Alex Krizhevsky、Hinton（获2019图灵奖）和Ilya Sutskever（Alpha GO、TensorFlow的开发者）设计的。原论文名为ImageNet Classification with Deep Convolutional Neural Networks（使用深度神经网络对ImageNet进行图像分类），"}</script><link rel="canonical" href="http://williamleung.cn/posts/12577bd1/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.15.2/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/stackoverflow-dark.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?31bf6a481783f6848cf7b5b51202f1dc";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-72437521-5" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-72437521-5');</script><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css"><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="WilliamLeung的个人博客" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/categories">按类别查看</a><a class="navbar-item" href="/tags">按标签查看</a><a class="navbar-item" href="/archives">按日期查看</a><a class="navbar-item" href="/posts/1/">关于</a></div><div class="navbar-end"><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-09-22T19:21:43.000Z" title="2021/9/23 03:21:43">2021-09-23</time>发表</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/">计算机</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></span><span class="level-item">31 分钟读完 (大约4676个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">AlexNet论文精读</h1><div class="content"><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/AlexNet/22689612?fr=aladdin">AlexNet</a>是2012年<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/ImageNet/17752829">ImageNet</a>竞赛冠军获得者Alex Krizhevsky、<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%9D%B0%E5%BC%97%E9%87%8C%C2%B7%E8%BE%9B%E9%A1%BF/23419046?fr=aladdin">Hinton</a>（获2019图灵奖）和Ilya Sutskever（Alpha GO、TensorFlow的开发者）设计的。原论文名为<em><strong>ImageNet Classification with Deep Convolutional Neural Networks</strong></em>（使用深度神经网络对ImageNet进行图像分类），是计算机视觉领域最为经典的论文之一，引用量至今已达1.4万次，主要提出者Alex Krizhevsky是人工智能教父Hinton的学生，这三位作者被称为人工智能三巨头。</p>
<p>因为AlexNet的提出，使得那一年之后，更多的更深的神经网络被提出，比如优秀的<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/VGG%20%E6%A8%A1%E5%9E%8B/22689655?fr=aladdin">VGG</a>,<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/GoogLeNet/22689587">GoogLeNet</a>，他们都是基于AlexNet的深度卷积神经网络来提取图像特征的。尽管AlexNet是2012年提出的模型，在如今已经有了更多的更优秀的模型，但是他们都是从AlexNet中得到启发。其首次将深度学习应用在大规模的图像分类上，在当时在产业界和学术界引发了很大的轰动。之后不管是图像分类，目标检测、语义分割都是在AlexNet的基础之上，AlexNet奠定了计算机视觉深度学习的基本模型方法和技巧。所以我认为AlexNet是学习计算机视觉必须了解的一个模型。</p>
<p>今天我将通过精读这篇论文原文的方式去了解AlexNet的一些基本思想和原理，下面我主要通过下面四个方面来对这篇论文进行精读。</p>
<ol>
<li>AlexNet 网络结构及参数计算</li>
<li>AlexNet 网络特色及训练技巧</li>
<li>实验设置及结果分析</li>
<li>论文总结</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/20201124162846687.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>AlexNet论文链接：<a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf</a></p>
<h3 id="文章目录"><a href="#文章目录" class="headerlink" title="文章目录"></a>文章目录</h3><ul>
<li><ul>
<li><ul>
<li>摘要</li>
<li>论文小标题</li>
<li>AlexNet 网络结构</li>
<li><ul>
<li> ReLU</li>
<li>Training on Multiple GPUs</li>
<li>Local Response Normalization（局部响应标准化）</li>
<li>Overlapping Pooling</li>
</ul>
</li>
<li>Reducing Overfitting</li>
<li>实验结果与分析</li>
<li>论文总结</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p>下面，我们首先来看看摘要。</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p><img src="https://img-blog.csdnimg.cn/20201124165216562.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p><strong>我们可以将摘要总结如下</strong>：</p>
<ul>
<li>在 ILSVRC-2010 的120万图片上训练深度卷积神经网络，获得最优结果，top-1 和 top-5 error 分别为 37.5% 和 17%。</li>
<li>在 Alexnet 中，一共有6千万个参数和65万个神经元，包括5个卷积层和3个全连接层。</li>
<li>为了训练的更快，Alexnet 使用了非饱和激活函数——ReLU，采用GPU进行训练</li>
<li>为了防止过拟合，在全连接层采取了 “dropout” 的方法。</li>
<li>基于以上技巧，在 ILSVRC-2012 以超出第二名10.9个百分点成绩夺冠。</li>
</ul>
<hr>
<h3 id="论文小标题"><a href="#论文小标题" class="headerlink" title="论文小标题"></a>论文小标题</h3><p>  看完了摘要，我们应该将这篇论文的小标题列举出来，通过观察这些小标题，我们大致就可以看出这篇论文的大致框架是怎样的。</p>
<p><img src="https://img-blog.csdnimg.cn/2020112416412246.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>  可以看出，我们应该重点关注第3部分，它的子标题也是最多的。我们可以看到 3.5 是一个总览框架，所以我们阅读时可以先看 3.5，再去看 3.1-3.4。</p>
<p>  下面我们就从 AlexNet 网络结构出发（3.5）。</p>
<hr>
<h3 id="AlexNet-网络结构"><a href="#AlexNet-网络结构" class="headerlink" title="AlexNet 网络结构"></a>AlexNet 网络结构</h3><p>  我们先看 3.5 的总体框架。</p>
<p><img src="https://img-blog.csdnimg.cn/2020112417105897.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p><strong>可以从上面两段文字总结如下</strong>：</p>
<ol>
<li>这个网络包括8个带权重的层，5个卷积层和3个全连接层。</li>
<li>第2，4，5层只和相同 GPU 上的前一层相连接。</li>
<li>第3层和第2层中所有 GPU 上的前一层相连。</li>
<li>在第1层和第2层后有 LRN 层。</li>
<li>最大池化层会在第1，2，5层之后。</li>
<li>ReLU函数会在所有的卷积层和全连接层。</li>
</ol>
<p>接下来我们看看对图的描述。</p>
<p><img src="https://img-blog.csdnimg.cn/20201124200434382.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  在文字中，神经网络的输入是 150,528 维度的，从图中就可以看出，输入是 224x224x3 = 150528。然后每层的神经元个数如下：</p>
<p><img src="https://img-blog.csdnimg.cn/20201124202040999.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>  我们按照之前的文字描述将原神经网络给描述得更清晰一些：</p>
<p><img src="https://img-blog.csdnimg.cn/20201124202948287.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  <br>  如果你还是不太明白，那么就看下面这张图：</p>
<p><img src="https://img-blog.csdnimg.cn/20201124204109920.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>  但是你看到第一层，你就可能会有一个疑问，输入层不是 224x224 吗，怎么变成了 227x227，那是因为如果将 224x224 代入 F o F_o<em>F**o</em> 公式计算会出现不能整除的情况，所以为了避免这种情况，应该将此处的 padding 设为 2，代入F o F_o<em>F**o</em> 公式可得到输出后的大小为 55*55。</p>
<p>  接下来，我们就来计算一下，这个网络怎么得到的 6000 万个参数。</p>
<p><img src="https://img-blog.csdnimg.cn/20201124205935514.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  <br>  我们通过上面的公式计算一下第一层的连接数量：<code>3 * 11 * 11 * 96 + 96 = 34944</code>，第二层的连接数为：<code>96 * 5 * 5 * 256 + 256 = 614656</code>，以此类推。但是我们观察到 FC1 层的连接数量为 37,752,832 个，占了整个连接数量的一半，所以全连接层是非常消耗内存的。</p>
<p>  我们现在已经整体的把握了 AlexNet 的网络结构，我们再倒回去看 3.1-3.4 的部分，来看看该网络到底是怎么实现的。</p>
<p><img src="https://img-blog.csdnimg.cn/20201124210856807.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  根据上面一段话可知，3.1-3.4 是按照重要程度依次递减排列的。那么我们将 3.1-3.4 的小标题再列出来：<br>  3.1 ReLU Nonlinearity（the most important）<br>  3.2 Training on Multiple GPUs<br>  3.3 Local Response Normalization（局部相应标准化）<br>  3.4 Overlapping Pooling</p>
<p>  </p>
<h4 id="3-1-ReLU"><a href="#3-1-ReLU" class="headerlink" title="3.1 ReLU"></a>3.1 ReLU</h4><p>下面，我们就来看看 3.1。<br><img src="https://img-blog.csdnimg.cn/20201124211256282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  <br>  上面一段话表明，通常我们模型的激活函数会选择 tanh 或者 sigmoid 函数，但是它们都是属于饱和函数，在这个模型中，采用的是<strong>非饱和非线性的激活函数</strong> —— ReLU 函数。因为就梯度下降的训练时间而言，饱和函数要比非饱和函数慢得多。可以从下图中观察得知：</p>
<p><img src="https://img-blog.csdnimg.cn/20201125073041214.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  从上图可以得知，当 Training error rate 下降到 0.25 时，ReLU 函数需要训练大概 6 个 epoch，而 tanh 要训练 36 个 epoch，两个函数差了 6 倍之多，可见该网络模型的成功离不开 ReLU 函数。</p>
<p>那么 ReLU 还有哪些优点呢？</p>
<ul>
<li>使网络训练更快</li>
<li>防止梯度消失</li>
<li>使网络具有稀疏性</li>
</ul>
<p>  下面就来看一下 ReLU 函数和 Sigmoid 函数曲线的对比：</p>
<p><img src="https://img-blog.csdnimg.cn/20201125074437311.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<blockquote>
<p><strong>补充：饱和函数</strong><br>  <img src="https://img-blog.csdnimg.cn/20201124211917693.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述">
  </p>
</blockquote>
<h4 id="3-2-Training-on-Multiple-GPUs"><a href="#3-2-Training-on-Multiple-GPUs" class="headerlink" title="3.2 Training on Multiple GPUs"></a>3.2 Training on Multiple GPUs</h4><p>  该模型主要使用了2个GPU，在模型总览那里我们已经了解了怎么使用多 GPU，这里就不再赘述了。</p>
<h4 id="3-3-Local-Response-Normalization（局部响应标准化）"><a href="#3-3-Local-Response-Normalization（局部响应标准化）" class="headerlink" title="3.3 Local Response Normalization（局部响应标准化）"></a>3.3 Local Response Normalization（局部响应标准化）</h4><p>  <strong>局部响应标准化</strong>：有助于 AlexNet <strong>泛化能力的提升</strong>，受真实神经元<strong>侧抑制</strong>（lateral inhibition）启发。</p>
<p>  <strong>侧抑制</strong>：细胞分化变为不同时，它会对周围细胞产生抑制信号，阻止它们向相同方向分化，最终表现为细胞命运的不同。</p>
<p>  在论文中提到了一个公式，下面我们就来解释一下这个公式：</p>
<p><img src="https://img-blog.csdnimg.cn/20201125075506464.png#pic_center" alt="在这里插入图片描述"></p>
<ul>
<li>b x , y i b_{x,y}^i<em>b**x</em>,<em>y**i</em>：表示神经元局部响应标准化后的值，i i<em>i</em> 表示通道，x , y x, y<em>x</em>,<em>y</em> 像素的位置</li>
<li>a x , y i a_{x,y}^i<em>a**x</em>,<em>y**i</em>：表示神经元局部响应标准化前的值</li>
<li>k k<em>k</em>：超参数，由原型中的 bias 指定</li>
<li>α α<em>α</em>：超参数，由原型中的 α α<em>α</em></li>
<li>N N<em>N</em>：每个特征图里面最内层向量的列数</li>
<li>β β<em>β</em>：超参数，由原型中的 β β<em>β</em> 指定</li>
<li>其中，k = 2 , n = 5 , α = 1 e − 4 , β = 0.75 k=2, n=5, α=1e-4, β=0.75<em>k</em>=2,<em>n</em>=5,<em>α</em>=1<em>e</em>−4,<em>β</em>=0.75</li>
</ul>
<p>  式子中的 max(0, i-n/2) 和 min(N-1, i+n/2) 是为了边界溢出的问题。我们真正需要关心的是 i-n/2 和 i+n/2。假设我们当前通道为 i i<em>i</em>，我们会往左考虑 n/2 个通道，向右考虑 n/2 个通道，这就对应了侧抑制概念中当细胞分化不同时，它会对<strong>周围细胞</strong>产生抑制信号。而左右 n/2 对应周围细胞。其实这个公式就是看分母，如果分母越大，那么最后得出的值相对就会比较小，就表现出抑制的作用。</p>
<h4 id="3-4-Overlapping-Pooling"><a href="#3-4-Overlapping-Pooling" class="headerlink" title="3.4 Overlapping Pooling"></a>3.4 Overlapping Pooling</h4><p>  我们先来看一下我们常见的池化方式是怎样的？</p>
<p><img src="https://img-blog.csdnimg.cn/20201125091952691.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  <br>  上图中输入特征图是 4<em>4 的，它的滑动窗口大小是 2</em>2 的，那么它会进行4次池化，而且每次池化的部分是不重叠的（每一部分用不同颜色区分）。通常，我们就会将移动的步长 s 设置为滑动窗口大小，即 s = 2。但是该模型中使用的是有重叠的池化，即 s &lt; 滑动窗口大小。</p>
<p><img src="https://img-blog.csdnimg.cn/20201125092540323.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>  以上就是神经网络大致结构。下面我们就来看看网络的训练技巧，这就是第 4 小节的内容了，Reducing Overfitting。</p>
<hr>
<h3 id="Reducing-Overfitting"><a href="#Reducing-Overfitting" class="headerlink" title="Reducing Overfitting"></a>Reducing Overfitting</h3><p>  这篇论文主要使用的减少过拟合的技巧有两个：Data Augmentation 和 Dropout。</p>
<p>  下面我们先来看看 4.1 Data Augmentation 主要使用了哪些方法。</p>
<p><img src="https://img-blog.csdnimg.cn/20201126081429194.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  <br>(1)：在训练阶段，从大小为 256<em>256 的图片中随机截取出 224</em>224 的图片，然后再经过水平翻转，我们可以得到 2048 张图片。<br>(2)：2048 = (256-224)^2 * 2 平方表示宽和高两方面，<em>2 表示水平翻转<br>(3)：在测试阶段采取的方法是从一张 256</em>256 的图片截取 5 张 224*224 的图片，分别从左上角，右上角，左下角，右下角，中心截取5张，然后分别进行水平翻转获得10张图片。</p>
<p><img src="https://img-blog.csdnimg.cn/20201126082104795.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  可见第一种方法使用的是裁剪和翻转来使数据量扩增。下面我们来看看第二种方法。</p>
<p><img src="https://img-blog.csdnimg.cn/20201126083025925.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  主要说的就是对<strong>图像颜色的扰动</strong>。</p>
<p>  下面我们总结一下针对 Data Augmentation 采取的两种方法：</p>
<p><img src="https://img-blog.csdnimg.cn/20201126083200580.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  <br>  说完了 Data Augmentation。再来看看 Dropout（随即失失活）。</p>
<p><img src="https://img-blog.csdnimg.cn/20201126090300131.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/2020112609024480.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<hr>
<h3 id="实验结果与分析"><a href="#实验结果与分析" class="headerlink" title="实验结果与分析"></a>实验结果与分析</h3><p>  我们主要分析三个部分：</p>
<p><img src="https://img-blog.csdnimg.cn/20201126090711770.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  <br>  下面先来看看第一部分，在 ILSVRC-2012 挑战赛中的成果。下表表示了 AlexNet 在 Top-1 和 Top-5 这两个指标上的变化情况，它采用了四种不同的方式来获取这些指标。我们要去学习网络不断改进的思路和方法。</p>
<p><img src="https://img-blog.csdnimg.cn/20201126091309315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述">
  </p>
<p>  接下来我们看 Qualitative Evaluations。</p>
<p><img src="https://img-blog.csdnimg.cn/2020112611002659.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20201126125530400.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  从上图的有右图可以得出特征的相似性：相似图片的第二个全连接层输出<strong>特征向量</strong>的欧氏距离<strong>相近</strong>。</p>
<p>  <strong>启发</strong>：可用 AlexNet 提取高级特征进行图像检索、图像聚类、图像编码。</p>
<hr>
<h3 id="论文总结"><a href="#论文总结" class="headerlink" title="论文总结"></a>论文总结</h3><p>  在研究完每篇论文之后，我们都需要做一个总结。主要从三方面进行总结：关键点、创新点和启发点。</p>
<p>  那么我们先来看一下这篇论文的总结是什么?</p>
<p><img src="https://img-blog.csdnimg.cn/20201126192043286.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  第一段其实主要想讲关于<strong>模型的网络结构之间的相关联</strong>是非常重要的，例如，我们移除了某一个神经网络的卷积层，这个模型的性能会下降大概2%。同样的，我们在使用网络的时候，也不要随意增加网络的层数。</p>
<p>  第二段主要是想讲<strong>未来的研究方向</strong>，这也是大部分论文在论文结尾都会点出来的，自己的不足或者未来可以去发展的方向。这里它提到未来可以用视频的数据来训练一个更大的<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&spm=1001.2101.3001.7020">卷积神经网络</a>。因为这里提到一些时序上的信息的缺失，AlexNet 都是基于 2D 的图片，是缺少时间这个维度的，而视频数据恰恰补充了这个信息，所以这里提出来了未来可以研究的方向就是基于视频信息来训练一个更大的卷积神经网络。</p>
<p>  下面就来进行自我总结。</p>
<p><strong>关键点</strong></p>
<ul>
<li>大量带标签的数据 —— ImageNet（算料）</li>
<li>高性能计算资源 —— GPU（算力）</li>
<li>合理算法模型 —— 深度卷积神经网络（算法）</li>
</ul>
<p><strong>创新点</strong></p>
<ul>
<li>采用 ReLU 加快大型神经网络训练</li>
<li>采用 LRN 提升大型网络泛化能力</li>
<li>采用 Overlapping Pooling 提升指标</li>
<li>采用随机裁剪翻转及色彩扰动增加数据多样性（重点）</li>
<li>采用 Dropout 减轻过拟合（FC层）</li>
</ul>
<p><strong>启发点</strong></p>
<ul>
<li><strong>深度与宽度可决定网络能力</strong><br>Their capacity can be controlled by varying their depth and breadth. （1 Introduction p2）</li>
<li>更强大的GPU及更多数据可进一步提高模型性能<br>All of our experiments suggest that our results<br>can be improved simply by waiting for faster GPUs and bigger datasets to become available.</li>
<li><strong>图片缩放细节，对短边先缩放</strong><br>Given a<br>rectangular image, we first rescaled the image such that the shorter side was of length 256, and then<br>cropped out the central 256×256 patch from the resulting image.（2 Dataset p3）</li>
<li><strong>ReLU 不需要对输入进行标准化来防止饱和现象，即说明 sigmoid/tanh 激活函数有必要对输入进行标准化</strong><br>ReLUs have the desirable property that they do not require input normalization to prevent them<br>from saturating. （3.3 LRN p1）</li>
</ul>
<hr>
</div><div class="article-licensing box"><div class="licensing-title"><p>AlexNet论文精读</p><p><a href="http://williamleung.cn/posts/12577bd1/">http://williamleung.cn/posts/12577bd1/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>William</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-09-23</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2022-05-27</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0</a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/AlexNet/">AlexNet</a><a class="link-muted mr-2" rel="tag" href="/tags/%E8%AE%BA%E6%96%87/">论文</a><a class="link-muted mr-2" rel="tag" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="link-muted mr-2" rel="tag" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=61486dd4f48362001928a658&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://s3.bmp.ovh/imgs/2021/09/87da4b64c8c15c26.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://s3.bmp.ovh/imgs/2021/09/010147419ee842be.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/posts/483895fb/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">VGG论文精读</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/posts/1/"><span class="level-item">关于个人博客网站的说明</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="SOHUCS" sid="posts/12577bd1/"></div><script charset="utf-8" src="https://changyan.sohu.com/upload/changyan.js"></script><script>window.changyan.api.config({appid: 'cyvFsKL3O',conf: 'prod_f74cae8cf60a3e05e786c3be168ebe03'});</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://img2.baidu.com/it/u=1470655022,1507167867&amp;fm=26&amp;fmt=auto" alt="William"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">William</p><p class="is-size-6 is-block">一个学计算机的大学生</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hangzhou, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">7</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/mrwilliamleung" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/mrwilliamleung"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="QQ" href="http://wpa.qq.com/msgrd?v=3&amp;uin=858920934&amp;site=qq&amp;menu=yes"><i class="fab fa-qq"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Instagram" href="https://www.instagram.com/mr_williamleung/"><i class="fab fa-instagram"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://www.Google.com/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Google</span></span><span class="level-right"><span class="level-item tag">www.google.com</span></span></a></li><li><a class="level is-mobile" href="https://www.baidu.com/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Baidu</span></span><span class="level-right"><span class="level-item tag">www.baidu.com</span></span></a></li><li><a class="level is-mobile" href="https://en.wikipedia.org/wiki/Main_Page" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Wiki</span></span><span class="level-right"><span class="level-item tag">en.wikipedia.org</span></span></a></li><li><a class="level is-mobile" href="https://www.bilibili.com/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bilibili</span></span><span class="level-right"><span class="level-item tag">www.bilibili.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/%E5%85%B3%E4%BA%8E/"><span class="level-start"><span class="level-item">关于</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"><span class="level-start"><span class="level-item">计算机</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"><span class="level-start"><span class="level-item">人工智能</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-27T14:15:13.000Z">2022-05-27</time></p><p class="title"><a href="/posts/483895fb/">VGG论文精读</a></p><p class="categories"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/">计算机</a> / <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-09-22T19:21:43.000Z">2021-09-23</time></p><p class="title"><a href="/posts/12577bd1/">AlexNet论文精读</a></p><p class="categories"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/">计算机</a> / <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-09-20T15:27:44.000Z">2021-09-20</time></p><p class="title"><a href="/posts/1/">关于个人博客网站的说明</a></p><p class="categories"><a href="/categories/%E5%85%B3%E4%BA%8E/">关于</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">五月 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">九月 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/AlexNet/"><span class="tag">AlexNet</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VGG/"><span class="tag">VGG</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"><span class="tag">个人博客</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="tag">深度学习</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><span class="tag">神经网络</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BA%E6%96%87/"><span class="tag">论文</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%B4%E6%98%8E/"><span class="tag">说明</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="WilliamLeung的个人博客" height="28"></a><p class="is-size-7"><span>&copy; 2022 William</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdnjs.loli.net/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>