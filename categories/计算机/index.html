<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>分类: 计算机 - WilliamLeung的个人博客</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#f7f7f7"><meta name="application-name" content="WilliamLeung的博客"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="msapplication-TileColor" content="#f7f7f7"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="WilliamLeung的博客"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="一个学习计算机的大学生的个人博客"><meta property="og:type" content="blog"><meta property="og:title" content="WilliamLeung的个人博客"><meta property="og:url" content="http://williamleung.cn/"><meta property="og:site_name" content="WilliamLeung的个人博客"><meta property="og:description" content="一个学习计算机的大学生的个人博客"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://williamleung.cn/img/og_image.png"><meta property="article:author" content="William"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://williamleung.cn"},"headline":"WilliamLeung的个人博客","image":["http://williamleung.cn/img/og_image.png"],"author":{"@type":"Person","name":"William"},"publisher":{"@type":"Organization","name":"WilliamLeung的个人博客","logo":{"@type":"ImageObject","url":"http://williamleung.cn/img/logo.svg"}},"description":"一个学习计算机的大学生的个人博客"}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.15.2/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/stackoverflow-dark.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?31bf6a481783f6848cf7b5b51202f1dc";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-72437521-5" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-72437521-5');</script><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css"><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="WilliamLeung的个人博客" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/categories">按类别查看</a><a class="navbar-item" href="/tags">按标签查看</a><a class="navbar-item" href="/archives">按日期查看</a><a class="navbar-item" href="/posts/1/">关于</a></div><div class="navbar-end"><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">分类</a></li><li class="is-active"><a href="#" aria-current="page">计算机</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-05-27T14:15:13.000Z" title="2022/5/27 22:15:13">2022-05-27</time>发表</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/">计算机</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></span><span class="level-item">34 分钟读完 (大约5161个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/posts/483895fb/">VGG论文精读</a></h1><div class="content"><h3 id="文章目录"><a href="#文章目录" class="headerlink" title="文章目录"></a>文章目录</h3><ul>
<li>题目</li>
<li>论文总览</li>
<li>Part 1：论文导读</li>
<li><ul>
<li>研究背景</li>
<li>研究成果</li>
</ul>
</li>
<li>论文精读</li>
<li><ul>
<li>摘要</li>
<li>论文小标题</li>
<li>VGGNet 网络结构</li>
<li>Classification Framework</li>
<li><ul>
<li>Training</li>
<li>Testing</li>
<li> Implementation Details</li>
</ul>
</li>
<li>Classification Experiments</li>
<li><ul>
<li> Single Scale Evaluation</li>
<li> Multi-Scale Evaluation</li>
<li> Multi-Crop Evaluation</li>
<li>Convent Fusion</li>
<li>Comparison With The State Of The Art</li>
</ul>
</li>
<li>论文总结</li>
</ul>
</li>
</ul>
<h1 id="题目："><a href="#题目：" class="headerlink" title="题目："></a>题目：</h1><p>  VggNet：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.1556">《Very Deep Convolutional Networks for Large-Scale Image Recognition》</a>，大规模图像识别的深度卷积网络。</p>
<h1 id="论文总览"><a href="#论文总览" class="headerlink" title="论文总览"></a>论文总览</h1><p>  首先，读一篇论文，我们第一遍通常需要泛读，即读这篇论文的题目，摘要，各个部分的标题和小标题，以及图片和表格。</p>
<p>  下面是泛读之后整理出该篇论文大致分成5个部分。</p>
<p><img src="https://img-blog.csdnimg.cn/2020113013431598.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<hr>
<h1 id="Part-1：论文导读"><a href="#Part-1：论文导读" class="headerlink" title="Part 1：论文导读"></a>Part 1：论文导读</h1><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>  VGGNet 这篇论文的主要贡献就是在于从网络<strong>深度</strong>这一角度出发，对卷积神经网络进行了改进。非常详细的评估了网络深度所带来的影响，证明了网络的深度对于性能的提升具有举足轻重的作用。而且由于文中训练的两个16层和19层的网络由于其强大的泛化能力，在之后得到了非常广泛的应用。</p>
<blockquote>
<p><strong>VGGNet 主要特点</strong>：</p>
<ul>
<li>网络很深</li>
<li>卷积层中使用的卷积核很小，且都是3*3的卷积核</li>
</ul>
</blockquote>
<hr>
<h2 id="研究成果"><a href="#研究成果" class="headerlink" title="研究成果"></a>研究成果</h2><p>  VGGNet 在2014年的 ImageNet Challenge 中获得了分类任务的第二名和定位任务的第一名。而且在分类任务中，只和获得冠军的 GoogLeNet 只相差了 0.1%，而且在单个网络中，VGG 是表现最好的，误差是 7.0%，比 GoogLeNet 的 7.9% 高了 0.9 个百分点。<br><img src="https://img-blog.csdnimg.cn/2020113014022218.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<hr>
<h1 id="Part-2：论文精读"><a href="#Part-2：论文精读" class="headerlink" title="Part 2：论文精读"></a>Part 2：论文精读</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>下面，我们先来看看摘要。<br><img src="https://img-blog.csdnimg.cn/20201130141052968.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>我们可以将摘要总结如下：</strong></p>
<ul>
<li>这篇论文主要研究的是卷积神经网络深度对分类准确度的影响</li>
<li>我们整个网络都使用 3*3 的卷积核，成功将深度扩大了16-19层，并且得到了一个很好的效果</li>
<li>VGGNet 获得了2014年 ImageNet Challenge 的定位任务第一名和分类任务第二名</li>
<li>该模型的泛化能力很强，运用到其它数据集上也得到了很好的效果</li>
<li>表现最好的两个模型已经投入到未来的研究中</li>
</ul>
<hr>
<h2 id="论文小标题"><a href="#论文小标题" class="headerlink" title="论文小标题"></a>论文小标题</h2><p><img src="https://img-blog.csdnimg.cn/20201130142347738.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>  从小标题的个数可以看出，第4部分的子标题最多，我们应该重点关注，其次应该关注第2、3部分。</p>
<p>  当我们正式进入某一部分时，我们可以先看一下，该部分有没有 Overall、Conclusion、Discussion等等。或许我们先从该部分入手，先了解到该部分的大体内容，再返回去看具体的实现细节。</p>
<p>  那么我们先从网络的架构（Configurations）入手，从上面的小标题可以看出，我们可以先从 2.3 小节入手。</p>
<hr>
<h2 id="2-VGGNet-网络结构"><a href="#2-VGGNet-网络结构" class="headerlink" title="2. VGGNet 网络结构"></a>2. VGGNet 网络结构</h2><p>我们先看 2.3 的 discussion。<br><img src="https://img-blog.csdnimg.cn/20201130143623572.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>可以从上面一段文字总结如下：</strong></p>
<ol>
<li>整个神经网络都是使用的 3*3的卷积核</li>
<li>通过堆叠多个3<em>3的卷积核来代替大尺度卷积核，堆积2个3</em>3的卷积代替一个5<em>5的卷积，堆叠3个3</em>3的卷积代替一个7*7的卷积</li>
<li>这样做的好处：不仅能使决策函数更有区别，而且使得参数大大减少</li>
</ol>
<hr>
<p><img src="https://img-blog.csdnimg.cn/20201130144421507.png" alt="在这里插入图片描述"></p>
<p>  <strong>可以从上面一段文字总结如下：</strong> 使用 1*1 的卷积可以在不影响卷积层的结果的情况下增加决策函数的非线性。</p>
<hr>
<p><img src="https://img-blog.csdnimg.cn/20201130145042462.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>  <strong>可以从上面一段文字总结如下：</strong> 这次比赛的冠军 GoogLeNet 也使用了非常深的网络（22层）和小的卷积核（1<em>1，3</em>3，5*5）。它们使用为了减少计算量，设计更为复杂，更积极地减少了第一层的特征图的空间分布。但是从单个网络的分类准确率来看，VGGNet 的表现要优于 GoogLeNet。</p>
<hr>
<p>接下来，我们再来看 2.3 小节中的图。<br><img src="https://img-blog.csdnimg.cn/20201130160204333.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>  <br>  在实验中，一共尝试了上面六种模型（每一列是一种模型），模型中都使用了是 3x3 的卷积核大小，在模型 C 中尝试了 1x1 的卷积核，模型的深度从左向右依次增加。为了简洁方便，表格中只展示了卷积操作和池化操作，没有显示激活函数（在每个卷积层和全连接层后都有激活函数）。而且我们发现每层的通道数很小，第一层的通道数只有64，之后每进行一次最大池化，通道数变为之前的2倍，直到增加到512为止。</p>
<p>  下面是16层的 VGGNet 的模型图。<br><img src="https://img-blog.csdnimg.cn/20201130214328120.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>图中符号表示：</strong></p>
<ul>
<li>conv3-64：卷积核的大小为 3*3，输出通道数为 64</li>
<li>conv1-256：卷积核大小为 1*1，输出通道数为 256</li>
<li>FC-4096：全连接层，输出神经元个数为 4096</li>
<li>FC-1000：全连接层，输出神经元个数为 1000</li>
<li>maxpool：采用最大池化</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20201130161358641.png" alt="在这里插入图片描述"></p>
<p>  关于Table 2 的描述，在 2.2 小节中，我们来看看 2.2 小节。<br><img src="https://img-blog.csdnimg.cn/20201130162326973.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>  Table 2 展示了上面六种模型中的参数个数，从 Table 1 中，我们知道模型的深度是从11层增加到了19层，尽管深度很深，但是网络的权重的数量是不大于一个更浅更宽的网络的。</p>
<p>  最后我们在来看看整体网络的架构（2.1）。<br><img src="https://img-blog.csdnimg.cn/20201130163021146.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>从上面几段话，可以<strong>总结如下</strong>：</p>
<ol>
<li>每个模型的输入都是 224x224 大小的RGB图片，而且需要在训练集上进行正则化。</li>
<li>在卷积层中使用的是 3x3 的卷积核，在模型 C 中使用了 1x1 的卷积核</li>
<li>六个模型都有5个最大池化层，并且池化层中使用的都是 2x2 的卷积核，并且 stride 为 2</li>
<li>所有的隐藏层都有激活函数</li>
<li>LRN（局部响应正则化）不能改善性能，反而会导致增加内存消耗和时间消耗</li>
</ol>
<hr>
<h2 id="3-Classification-Framework"><a href="#3-Classification-Framework" class="headerlink" title="3. Classification Framework"></a>3. Classification Framework</h2><p>  下面我们接着看第3节，该节主要描述的是网络中训练和评估的细节。我们先从 3.1 的 Training 开始。</p>
<h3 id="3-1-Training"><a href="#3-1-Training" class="headerlink" title="3.1 Training"></a>3.1 Training</h3><p><img src="https://img-blog.csdnimg.cn/20201130191846679.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>以上几段主要介绍了在训练过程中参数的设置以及初始化，以及扩充数据集的方法。总结如下：</p>
<ul>
<li>batch_size = 256</li>
<li>momentum = 0.9</li>
<li>L 2 L_2<em>L</em>2 正则项：5 ⋅ 1 0 − 4 5·10^{-4}5⋅10−4</li>
<li>dropout：p = 0.5</li>
<li>learning rate：0.01（当验证集准确度不变时，learning rate 变为原来的 1/10）</li>
<li>随机初始化权重，然后用模型 A 进行训练，然后将优化后得到的权重作为更深层的网络的前4层卷积层和最后3个全连接层的初始化权重，并且这些层的学习率不进行衰减。其余层进行进行随机初始化，权重初始化为均值为0，方差为0.01的正态分布，偏差为0。</li>
<li>但是他们在论文提交之后发现，可以不用上面的方法初始化权重，直接使用 Glorot &amp; Bengio(2010) 的随机初始化程序就能得到很好的效果。下面是 Glorot &amp; Bengio 的初始化方法。<br><img src="https://img-blog.csdnimg.cn/20201130211948621.png" alt="在这里插入图片描述"></li>
<li>他们使用随即裁剪的方式将输入的图片大小固定在 224x224。为了扩充数据集，采用<strong>随机水平翻转和随机RGB颜色扰动</strong></li>
</ul>
<p>下面是对于训练图片大小的设置。<br><img src="https://img-blog.csdnimg.cn/20201130193034861.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>上面采取了两种方法来设置训练图片的大小 S。</p>
<ol>
<li>第一种方法：<strong>固定 S</strong>，在实验中，评估模型用了两种图片大小：256 和 384。第一次训练固定图像大小 S = 256，为了加快 S = 384 的网络，将 S = 256 模型训练得到的参数作为 S = 384 模型的初始化权重，初始化学习率为 0.001</li>
<li>第二种方法：<strong>多尺度的训练</strong>。不固定训练图片的大小，将其固定在一个范围中 [256, 512]，在训练时，考虑到不同尺度的图片作为训练集训练网络对训练是有益的，也可以看做通过尺度抖动增加训练数据集。这样训练出来的模型可以识别各种大小的图片，由于速度方面的原因，我们训练多尺度模型的方法是对相同配置的单尺度模型的所有层进行微调，预先用固定的S = 384进行训练。</li>
</ol>
<h3 id="3-2-Testing"><a href="#3-2-Testing" class="headerlink" title="3.2 Testing"></a>3.2 Testing</h3><p>下面让我们来看一下测试数据集。<br><img src="https://img-blog.csdnimg.cn/20201130205155712.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>总结如下：</strong></p>
<ol>
<li>用一张测试图片的不同大小进行测试，最后取这些结果的平均值作为该图片的结果，这样也会改善性能。</li>
<li>在最后一个卷积层的最后我们需要做一个最大/平均池化，为了使得能够和全连接层连接上，所以需要将最后一个卷积层的输出进行规定。</li>
</ol>
<p>  举个栗子：假设输入的图片大小为 224x224x3，那么最后一个卷积层的输出为 7x7x512。那么如果输入图片大小为 448x448x3，那么最后一个卷积层的输出为 14x14x512。这样两个不同的尺度就不能连接同一个全连接层，所以需要对最后一个卷积层的输出做一个规定大小。那么只需要对他们做一个均值（最大）池化操作，7x7x512 池化之后就是 512，14x14x512 池化之后也是 512，这样就能与全连接层相连接了。</p>
<p><img src="https://img-blog.csdnimg.cn/20201130214450657.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<ol>
<li>通过<strong>水平翻转图像增加扩充测试数据集</strong>，最后将原始图片和翻转后的图片的结果平均值作为该图片的最终结果。</li>
<li>我们在评估网络时，把每张图片变为3个尺寸，从每个尺寸的图片中随即裁剪出50个不同的图片，然后一张图片就变成了150张图片
  </li>
</ol>
<h3 id="3-3-Implementation-Details"><a href="#3-3-Implementation-Details" class="headerlink" title="3.3 Implementation Details"></a>3.3 Implementation Details</h3><p>最后我们再来看看实现细节（3.3）<br><img src="https://img-blog.csdnimg.cn/20201130220235636.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>总结：</strong></p>
<p>  网络进行训练和评价以及训练和评估全尺寸多尺度图像都是使用一个系统上的多个 GPU 。多 GPU 训练利用数据并行性，将每批训练图像分割成若干 GPU 批次，在每个GPU上并行处理。计算完 GPU 批处理梯度后，取其平均值，得到整个批处理的梯度。梯度计算是同步的跨 GPU，因此结果和单一 GPU 训练是完全相同的。</p>
<p>   网络在4个 NVIDIA Titan Black GPUs 训练单个网络花费了 2-3 周。</p>
<hr>
<h2 id="4-Classification-Experiments"><a href="#4-Classification-Experiments" class="headerlink" title="4. Classification Experiments"></a>4. Classification Experiments</h2><p>  在第4节中，主要呈现了在 ILSVRC-2012 数据集上的分类结果，分类表现主要有两种评价指标：top-1 and top-5 errro，top-5 error 是 ILSVRC 的主要评价指标。</p>
<h3 id="4-1-Single-Scale-Evaluation"><a href="#4-1-Single-Scale-Evaluation" class="headerlink" title="4.1 Single Scale Evaluation"></a>4.1 Single <a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=Scale&spm=1001.2101.3001.7020">Scale</a> Evaluation</h3><p>  单尺度评估。首先我们来看看4.1小节给出的表格。<br><img src="https://img-blog.csdnimg.cn/20201201074414455.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>  对于模型 A，A-LRN，B，它们都采用的是固定 S = 256，并且我们发现模型 A-LRN 的表现还没有模型 A 好，所以对于后面的模型，都没有使用 LRN（局部响应正则化），但是我们观察到从模型 A 到模型 B，<strong>随着深度的增加，top-1 和 top-5 error 也在下降</strong>。</p>
<p>  再来观察模型 C、D、E，它们的 train 都测试了3种图片尺度，分别是 S=256，S=384，S=[256;512]。我们只看三个模型的 S=256 和 S=384，我们很容易发现不管哪一种模型，当 S=384，其模型的 top-1 和 top-5 是要优于 S=256 的。那是因为<strong>图片的分辨率越高，我们能够更容易捕捉到一些空间特征，所以其分类准确度就越高</strong>。</p>
<p>  最后，我们再来看看的三种模型的 S=[256;512] 的这种情况，我们很容易发现这种情况的分类结果不管在哪一种模型中都是表现最好的。这也证明了<strong>通过尺度来扩充训练集确实有助于捕获多尺度图像统计</strong>。</p>
<h3 id="4-2-Multi-Scale-Evaluation"><a href="#4-2-Multi-Scale-Evaluation" class="headerlink" title="4.2 Multi-Scale Evaluation"></a>4.2 Multi-Scale Evaluation</h3><p>  多尺度评估。主要评估在测试阶段尺度抖动对分类准确度影响。当用固定图片大小S进行训练得到的模型，会用一张测试图片的三种尺寸大小进行评估，那么三种图片大小分别为：S-32，S，S+32。当训练时不是使用的固定 S 进行训练，S 是一个变量，属于 [S m i n S_{min}<em>S<strong>m</strong>i**n</em>, S m a x S_{max}<em>S<strong>m</strong>a**x</em>]，那么我们在评估时，测试图片的大小分别是：S m i n S_{min}<em>S<strong>m</strong>i**n</em>, 0.5 ( S m i n + S m a x ) 0.5(S_{min} + S_{max})0.5(<em>S<strong>m</strong>i**n</em>+<em>S<strong>m</strong>a**x</em>), S m a x S_{max}<em>S<strong>m</strong>a**x</em>。</p>
<p>  评估结果如下：<br><img src="https://img-blog.csdnimg.cn/20201201082352597.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>  根据表格我们可以得出结论：在<strong>测试阶段的尺度抖动相较于单尺度的相同模型会有更好的表现</strong>。其中模型 D 和 E 表现最好。表现最好的单个网络在验证数据集上 top-1 和 top-5 error 达到了 24.8%/7.5%，在测试数据集上，模型 E 达到了 7.3 % 的 top-5 error。</p>
<h3 id="4-3-Multi-Crop-Evaluation"><a href="#4-3-Multi-Crop-Evaluation" class="headerlink" title="4.3 Multi-Crop Evaluation"></a>4.3 Multi-Crop Evaluation</h3><p>  多裁剪评估。这里主要用到两种评估方法，一种是 dense，即评估时所使用的图片是整张图片，不经过任何裁剪。那么另一种就是 multi-crop，就是评估时使用的是裁剪后的图片。评估结果如下表：<br><img src="https://img-blog.csdnimg.cn/20201201083757438.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>  从评估结果来看，<strong>使用 multi-crop 要比使用 dense 的表现稍微好一些</strong>，而且两种方法结合使用的表现比单独使用任何一种方法的效果都要好，因为两种方法是互补的。
  </p>
<h3 id="4-4-Convent-Fusion"><a href="#4-4-Convent-Fusion" class="headerlink" title="4.4 Convent Fusion"></a>4.4 Convent Fusion</h3><p>  模型融合。通过融合几种模型，最后取各个模型的 soft-max 结果的平均值作为模型的输出。下面是融合不同模型得到的实验结果。<br><img src="https://img-blog.csdnimg.cn/20201201084753151.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>  可以从实验结果发现，融合一个模型 D 和模型 E ，并且在评估时使用 multi-crop 和 dense 的方法得到的表现最好。</p>
<p>  </p>
<h3 id="4-5-Comparison-With-The-State-Of-The-Art"><a href="#4-5-Comparison-With-The-State-Of-The-Art" class="headerlink" title="4.5 Comparison With The State Of The Art"></a>4.5 Comparison With The State Of The Art</h3><p>  主要是和 ILSVRC 比赛中表现比较好的模型进行比较，下表是在挑战赛中表现比较好的模型。<br><img src="https://img-blog.csdnimg.cn/20201201085309296.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>  在 2014 年的挑战赛中，GoogLeNet 以 6.7% 的 error 夺冠，VGGNet 以 6.8% 的 error 获得第二名。但是如果从一个网络的分类准确度来看，VGG 是以 7.0% 的 error 要优于 GoogLeNet 的 7.9%。</p>
<hr>
<h2 id="论文总结"><a href="#论文总结" class="headerlink" title="论文总结"></a>论文总结</h2><p>  在研究完每篇论文之后，我们都需要做一个总结。</p>
<p>  那么我们先来看一下这篇论文的总结是什么?<br><img src="https://img-blog.csdnimg.cn/20201201090134263.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>再次强调了<strong>深度确实能够增加分类准确度</strong>。</p>
<hr>
<p>下面就来进行自我总结。</p>
<p><strong>创新点</strong></p>
<ul>
<li>整个网络都采用 3x3 的卷积核，从而增加神经网络的深度。两个3x3卷积核的堆叠代替一个5x5卷积核，三个3x3卷积核代替一个7x7卷积核。这样一方面能够减少参数的数量，另一方面拥有更多的非线性变化。</li>
<li>在卷积结构中引入1x1的卷积核，在不影响输入输出维度的情况下，引入非线性变换，增加网络的表达能力，降低计算量</li>
<li>通过预训练的方式来更好的初始化权重，加快训练的收敛速度</li>
<li>采用 Multi-Scale 的方式训练和预测，可以扩充数据集，防止过拟合，提升预测准确率。</li>
<li>深层网络更适合于大的数据集</li>
</ul>
<p><strong>启发点</strong></p>
<ul>
<li>深度能够提高网络的分类准确率</li>
<li>为了加快收敛速度，可以使用预训练的方式初始化权重</li>
<li>在更深层的网络中，LRN方法并没有什么用，反而会导致内存和时间的消耗</li>
<li>通过堆叠小卷积核可以减少网络参数，增加网络深度，提升网络性能</li>
<li>在训练和测试使用 Multi-Scale 可以扩充数据集，防止过拟合</li>
</ul>
<hr>
<p> </p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-09-22T19:21:43.000Z" title="2021/9/23 03:21:43">2021-09-23</time>发表</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/">计算机</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></span><span class="level-item">31 分钟读完 (大约4676个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/posts/12577bd1/">AlexNet论文精读</a></h1><div class="content"><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/AlexNet/22689612?fr=aladdin">AlexNet</a>是2012年<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/ImageNet/17752829">ImageNet</a>竞赛冠军获得者Alex Krizhevsky、<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%9D%B0%E5%BC%97%E9%87%8C%C2%B7%E8%BE%9B%E9%A1%BF/23419046?fr=aladdin">Hinton</a>（获2019图灵奖）和Ilya Sutskever（Alpha GO、TensorFlow的开发者）设计的。原论文名为<em><strong>ImageNet Classification with Deep Convolutional Neural Networks</strong></em>（使用深度神经网络对ImageNet进行图像分类），是计算机视觉领域最为经典的论文之一，引用量至今已达1.4万次，主要提出者Alex Krizhevsky是人工智能教父Hinton的学生，这三位作者被称为人工智能三巨头。</p>
<p>因为AlexNet的提出，使得那一年之后，更多的更深的神经网络被提出，比如优秀的<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/VGG%20%E6%A8%A1%E5%9E%8B/22689655?fr=aladdin">VGG</a>,<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/GoogLeNet/22689587">GoogLeNet</a>，他们都是基于AlexNet的深度卷积神经网络来提取图像特征的。尽管AlexNet是2012年提出的模型，在如今已经有了更多的更优秀的模型，但是他们都是从AlexNet中得到启发。其首次将深度学习应用在大规模的图像分类上，在当时在产业界和学术界引发了很大的轰动。之后不管是图像分类，目标检测、语义分割都是在AlexNet的基础之上，AlexNet奠定了计算机视觉深度学习的基本模型方法和技巧。所以我认为AlexNet是学习计算机视觉必须了解的一个模型。</p>
<p>今天我将通过精读这篇论文原文的方式去了解AlexNet的一些基本思想和原理，下面我主要通过下面四个方面来对这篇论文进行精读。</p>
<ol>
<li>AlexNet 网络结构及参数计算</li>
<li>AlexNet 网络特色及训练技巧</li>
<li>实验设置及结果分析</li>
<li>论文总结</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/20201124162846687.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>AlexNet论文链接：<a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf</a></p>
<h3 id="文章目录"><a href="#文章目录" class="headerlink" title="文章目录"></a>文章目录</h3><ul>
<li><ul>
<li><ul>
<li>摘要</li>
<li>论文小标题</li>
<li>AlexNet 网络结构</li>
<li><ul>
<li> ReLU</li>
<li>Training on Multiple GPUs</li>
<li>Local Response Normalization（局部响应标准化）</li>
<li>Overlapping Pooling</li>
</ul>
</li>
<li>Reducing Overfitting</li>
<li>实验结果与分析</li>
<li>论文总结</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p>下面，我们首先来看看摘要。</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p><img src="https://img-blog.csdnimg.cn/20201124165216562.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p><strong>我们可以将摘要总结如下</strong>：</p>
<ul>
<li>在 ILSVRC-2010 的120万图片上训练深度卷积神经网络，获得最优结果，top-1 和 top-5 error 分别为 37.5% 和 17%。</li>
<li>在 Alexnet 中，一共有6千万个参数和65万个神经元，包括5个卷积层和3个全连接层。</li>
<li>为了训练的更快，Alexnet 使用了非饱和激活函数——ReLU，采用GPU进行训练</li>
<li>为了防止过拟合，在全连接层采取了 “dropout” 的方法。</li>
<li>基于以上技巧，在 ILSVRC-2012 以超出第二名10.9个百分点成绩夺冠。</li>
</ul>
<hr>
<h3 id="论文小标题"><a href="#论文小标题" class="headerlink" title="论文小标题"></a>论文小标题</h3><p>  看完了摘要，我们应该将这篇论文的小标题列举出来，通过观察这些小标题，我们大致就可以看出这篇论文的大致框架是怎样的。</p>
<p><img src="https://img-blog.csdnimg.cn/2020112416412246.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>  可以看出，我们应该重点关注第3部分，它的子标题也是最多的。我们可以看到 3.5 是一个总览框架，所以我们阅读时可以先看 3.5，再去看 3.1-3.4。</p>
<p>  下面我们就从 AlexNet 网络结构出发（3.5）。</p>
<hr>
<h3 id="AlexNet-网络结构"><a href="#AlexNet-网络结构" class="headerlink" title="AlexNet 网络结构"></a>AlexNet 网络结构</h3><p>  我们先看 3.5 的总体框架。</p>
<p><img src="https://img-blog.csdnimg.cn/2020112417105897.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p><strong>可以从上面两段文字总结如下</strong>：</p>
<ol>
<li>这个网络包括8个带权重的层，5个卷积层和3个全连接层。</li>
<li>第2，4，5层只和相同 GPU 上的前一层相连接。</li>
<li>第3层和第2层中所有 GPU 上的前一层相连。</li>
<li>在第1层和第2层后有 LRN 层。</li>
<li>最大池化层会在第1，2，5层之后。</li>
<li>ReLU函数会在所有的卷积层和全连接层。</li>
</ol>
<p>接下来我们看看对图的描述。</p>
<p><img src="https://img-blog.csdnimg.cn/20201124200434382.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  在文字中，神经网络的输入是 150,528 维度的，从图中就可以看出，输入是 224x224x3 = 150528。然后每层的神经元个数如下：</p>
<p><img src="https://img-blog.csdnimg.cn/20201124202040999.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>  我们按照之前的文字描述将原神经网络给描述得更清晰一些：</p>
<p><img src="https://img-blog.csdnimg.cn/20201124202948287.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  <br>  如果你还是不太明白，那么就看下面这张图：</p>
<p><img src="https://img-blog.csdnimg.cn/20201124204109920.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>  但是你看到第一层，你就可能会有一个疑问，输入层不是 224x224 吗，怎么变成了 227x227，那是因为如果将 224x224 代入 F o F_o<em>F**o</em> 公式计算会出现不能整除的情况，所以为了避免这种情况，应该将此处的 padding 设为 2，代入F o F_o<em>F**o</em> 公式可得到输出后的大小为 55*55。</p>
<p>  接下来，我们就来计算一下，这个网络怎么得到的 6000 万个参数。</p>
<p><img src="https://img-blog.csdnimg.cn/20201124205935514.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  <br>  我们通过上面的公式计算一下第一层的连接数量：<code>3 * 11 * 11 * 96 + 96 = 34944</code>，第二层的连接数为：<code>96 * 5 * 5 * 256 + 256 = 614656</code>，以此类推。但是我们观察到 FC1 层的连接数量为 37,752,832 个，占了整个连接数量的一半，所以全连接层是非常消耗内存的。</p>
<p>  我们现在已经整体的把握了 AlexNet 的网络结构，我们再倒回去看 3.1-3.4 的部分，来看看该网络到底是怎么实现的。</p>
<p><img src="https://img-blog.csdnimg.cn/20201124210856807.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  根据上面一段话可知，3.1-3.4 是按照重要程度依次递减排列的。那么我们将 3.1-3.4 的小标题再列出来：<br>  3.1 ReLU Nonlinearity（the most important）<br>  3.2 Training on Multiple GPUs<br>  3.3 Local Response Normalization（局部相应标准化）<br>  3.4 Overlapping Pooling</p>
<p>  </p>
<h4 id="3-1-ReLU"><a href="#3-1-ReLU" class="headerlink" title="3.1 ReLU"></a>3.1 ReLU</h4><p>下面，我们就来看看 3.1。<br><img src="https://img-blog.csdnimg.cn/20201124211256282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  <br>  上面一段话表明，通常我们模型的激活函数会选择 tanh 或者 sigmoid 函数，但是它们都是属于饱和函数，在这个模型中，采用的是<strong>非饱和非线性的激活函数</strong> —— ReLU 函数。因为就梯度下降的训练时间而言，饱和函数要比非饱和函数慢得多。可以从下图中观察得知：</p>
<p><img src="https://img-blog.csdnimg.cn/20201125073041214.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  从上图可以得知，当 Training error rate 下降到 0.25 时，ReLU 函数需要训练大概 6 个 epoch，而 tanh 要训练 36 个 epoch，两个函数差了 6 倍之多，可见该网络模型的成功离不开 ReLU 函数。</p>
<p>那么 ReLU 还有哪些优点呢？</p>
<ul>
<li>使网络训练更快</li>
<li>防止梯度消失</li>
<li>使网络具有稀疏性</li>
</ul>
<p>  下面就来看一下 ReLU 函数和 Sigmoid 函数曲线的对比：</p>
<p><img src="https://img-blog.csdnimg.cn/20201125074437311.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<blockquote>
<p><strong>补充：饱和函数</strong><br>  <img src="https://img-blog.csdnimg.cn/20201124211917693.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述">
  </p>
</blockquote>
<h4 id="3-2-Training-on-Multiple-GPUs"><a href="#3-2-Training-on-Multiple-GPUs" class="headerlink" title="3.2 Training on Multiple GPUs"></a>3.2 Training on Multiple GPUs</h4><p>  该模型主要使用了2个GPU，在模型总览那里我们已经了解了怎么使用多 GPU，这里就不再赘述了。</p>
<h4 id="3-3-Local-Response-Normalization（局部响应标准化）"><a href="#3-3-Local-Response-Normalization（局部响应标准化）" class="headerlink" title="3.3 Local Response Normalization（局部响应标准化）"></a>3.3 Local Response Normalization（局部响应标准化）</h4><p>  <strong>局部响应标准化</strong>：有助于 AlexNet <strong>泛化能力的提升</strong>，受真实神经元<strong>侧抑制</strong>（lateral inhibition）启发。</p>
<p>  <strong>侧抑制</strong>：细胞分化变为不同时，它会对周围细胞产生抑制信号，阻止它们向相同方向分化，最终表现为细胞命运的不同。</p>
<p>  在论文中提到了一个公式，下面我们就来解释一下这个公式：</p>
<p><img src="https://img-blog.csdnimg.cn/20201125075506464.png#pic_center" alt="在这里插入图片描述"></p>
<ul>
<li>b x , y i b_{x,y}^i<em>b**x</em>,<em>y**i</em>：表示神经元局部响应标准化后的值，i i<em>i</em> 表示通道，x , y x, y<em>x</em>,<em>y</em> 像素的位置</li>
<li>a x , y i a_{x,y}^i<em>a**x</em>,<em>y**i</em>：表示神经元局部响应标准化前的值</li>
<li>k k<em>k</em>：超参数，由原型中的 bias 指定</li>
<li>α α<em>α</em>：超参数，由原型中的 α α<em>α</em></li>
<li>N N<em>N</em>：每个特征图里面最内层向量的列数</li>
<li>β β<em>β</em>：超参数，由原型中的 β β<em>β</em> 指定</li>
<li>其中，k = 2 , n = 5 , α = 1 e − 4 , β = 0.75 k=2, n=5, α=1e-4, β=0.75<em>k</em>=2,<em>n</em>=5,<em>α</em>=1<em>e</em>−4,<em>β</em>=0.75</li>
</ul>
<p>  式子中的 max(0, i-n/2) 和 min(N-1, i+n/2) 是为了边界溢出的问题。我们真正需要关心的是 i-n/2 和 i+n/2。假设我们当前通道为 i i<em>i</em>，我们会往左考虑 n/2 个通道，向右考虑 n/2 个通道，这就对应了侧抑制概念中当细胞分化不同时，它会对<strong>周围细胞</strong>产生抑制信号。而左右 n/2 对应周围细胞。其实这个公式就是看分母，如果分母越大，那么最后得出的值相对就会比较小，就表现出抑制的作用。</p>
<h4 id="3-4-Overlapping-Pooling"><a href="#3-4-Overlapping-Pooling" class="headerlink" title="3.4 Overlapping Pooling"></a>3.4 Overlapping Pooling</h4><p>  我们先来看一下我们常见的池化方式是怎样的？</p>
<p><img src="https://img-blog.csdnimg.cn/20201125091952691.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  <br>  上图中输入特征图是 4<em>4 的，它的滑动窗口大小是 2</em>2 的，那么它会进行4次池化，而且每次池化的部分是不重叠的（每一部分用不同颜色区分）。通常，我们就会将移动的步长 s 设置为滑动窗口大小，即 s = 2。但是该模型中使用的是有重叠的池化，即 s &lt; 滑动窗口大小。</p>
<p><img src="https://img-blog.csdnimg.cn/20201125092540323.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>  以上就是神经网络大致结构。下面我们就来看看网络的训练技巧，这就是第 4 小节的内容了，Reducing Overfitting。</p>
<hr>
<h3 id="Reducing-Overfitting"><a href="#Reducing-Overfitting" class="headerlink" title="Reducing Overfitting"></a>Reducing Overfitting</h3><p>  这篇论文主要使用的减少过拟合的技巧有两个：Data Augmentation 和 Dropout。</p>
<p>  下面我们先来看看 4.1 Data Augmentation 主要使用了哪些方法。</p>
<p><img src="https://img-blog.csdnimg.cn/20201126081429194.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  <br>(1)：在训练阶段，从大小为 256<em>256 的图片中随机截取出 224</em>224 的图片，然后再经过水平翻转，我们可以得到 2048 张图片。<br>(2)：2048 = (256-224)^2 * 2 平方表示宽和高两方面，<em>2 表示水平翻转<br>(3)：在测试阶段采取的方法是从一张 256</em>256 的图片截取 5 张 224*224 的图片，分别从左上角，右上角，左下角，右下角，中心截取5张，然后分别进行水平翻转获得10张图片。</p>
<p><img src="https://img-blog.csdnimg.cn/20201126082104795.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  可见第一种方法使用的是裁剪和翻转来使数据量扩增。下面我们来看看第二种方法。</p>
<p><img src="https://img-blog.csdnimg.cn/20201126083025925.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  主要说的就是对<strong>图像颜色的扰动</strong>。</p>
<p>  下面我们总结一下针对 Data Augmentation 采取的两种方法：</p>
<p><img src="https://img-blog.csdnimg.cn/20201126083200580.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  <br>  说完了 Data Augmentation。再来看看 Dropout（随即失失活）。</p>
<p><img src="https://img-blog.csdnimg.cn/20201126090300131.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/2020112609024480.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<hr>
<h3 id="实验结果与分析"><a href="#实验结果与分析" class="headerlink" title="实验结果与分析"></a>实验结果与分析</h3><p>  我们主要分析三个部分：</p>
<p><img src="https://img-blog.csdnimg.cn/20201126090711770.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  <br>  下面先来看看第一部分，在 ILSVRC-2012 挑战赛中的成果。下表表示了 AlexNet 在 Top-1 和 Top-5 这两个指标上的变化情况，它采用了四种不同的方式来获取这些指标。我们要去学习网络不断改进的思路和方法。</p>
<p><img src="https://img-blog.csdnimg.cn/20201126091309315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述">
  </p>
<p>  接下来我们看 Qualitative Evaluations。</p>
<p><img src="https://img-blog.csdnimg.cn/2020112611002659.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20201126125530400.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  从上图的有右图可以得出特征的相似性：相似图片的第二个全连接层输出<strong>特征向量</strong>的欧氏距离<strong>相近</strong>。</p>
<p>  <strong>启发</strong>：可用 AlexNet 提取高级特征进行图像检索、图像聚类、图像编码。</p>
<hr>
<h3 id="论文总结"><a href="#论文总结" class="headerlink" title="论文总结"></a>论文总结</h3><p>  在研究完每篇论文之后，我们都需要做一个总结。主要从三方面进行总结：关键点、创新点和启发点。</p>
<p>  那么我们先来看一下这篇论文的总结是什么?</p>
<p><img src="https://img-blog.csdnimg.cn/20201126192043286.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDMzMDEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>  第一段其实主要想讲关于<strong>模型的网络结构之间的相关联</strong>是非常重要的，例如，我们移除了某一个神经网络的卷积层，这个模型的性能会下降大概2%。同样的，我们在使用网络的时候，也不要随意增加网络的层数。</p>
<p>  第二段主要是想讲<strong>未来的研究方向</strong>，这也是大部分论文在论文结尾都会点出来的，自己的不足或者未来可以去发展的方向。这里它提到未来可以用视频的数据来训练一个更大的<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&spm=1001.2101.3001.7020">卷积神经网络</a>。因为这里提到一些时序上的信息的缺失，AlexNet 都是基于 2D 的图片，是缺少时间这个维度的，而视频数据恰恰补充了这个信息，所以这里提出来了未来可以研究的方向就是基于视频信息来训练一个更大的卷积神经网络。</p>
<p>  下面就来进行自我总结。</p>
<p><strong>关键点</strong></p>
<ul>
<li>大量带标签的数据 —— ImageNet（算料）</li>
<li>高性能计算资源 —— GPU（算力）</li>
<li>合理算法模型 —— 深度卷积神经网络（算法）</li>
</ul>
<p><strong>创新点</strong></p>
<ul>
<li>采用 ReLU 加快大型神经网络训练</li>
<li>采用 LRN 提升大型网络泛化能力</li>
<li>采用 Overlapping Pooling 提升指标</li>
<li>采用随机裁剪翻转及色彩扰动增加数据多样性（重点）</li>
<li>采用 Dropout 减轻过拟合（FC层）</li>
</ul>
<p><strong>启发点</strong></p>
<ul>
<li><strong>深度与宽度可决定网络能力</strong><br>Their capacity can be controlled by varying their depth and breadth. （1 Introduction p2）</li>
<li>更强大的GPU及更多数据可进一步提高模型性能<br>All of our experiments suggest that our results<br>can be improved simply by waiting for faster GPUs and bigger datasets to become available.</li>
<li><strong>图片缩放细节，对短边先缩放</strong><br>Given a<br>rectangular image, we first rescaled the image such that the shorter side was of length 256, and then<br>cropped out the central 256×256 patch from the resulting image.（2 Dataset p3）</li>
<li><strong>ReLU 不需要对输入进行标准化来防止饱和现象，即说明 sigmoid/tanh 激活函数有必要对输入进行标准化</strong><br>ReLUs have the desirable property that they do not require input normalization to prevent them<br>from saturating. （3.3 LRN p1）</li>
</ul>
<hr>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="William"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">William</p><p class="is-size-6 is-block">一个学计算机的大学生</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hangzhou, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">7</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/mrwilliamleung" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/mrwilliamleung"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="QQ" href="http://wpa.qq.com/msgrd?v=3&amp;uin=858920934&amp;site=qq&amp;menu=yes"><i class="fab fa-qq"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Instagram" href="https://www.instagram.com/mr_williamleung/"><i class="fab fa-instagram"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://www.Google.com/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Google</span></span><span class="level-right"><span class="level-item tag">www.google.com</span></span></a></li><li><a class="level is-mobile" href="https://www.baidu.com/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Baidu</span></span><span class="level-right"><span class="level-item tag">www.baidu.com</span></span></a></li><li><a class="level is-mobile" href="https://en.wikipedia.org/wiki/Main_Page" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Wiki</span></span><span class="level-right"><span class="level-item tag">en.wikipedia.org</span></span></a></li><li><a class="level is-mobile" href="https://www.bilibili.com/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bilibili</span></span><span class="level-right"><span class="level-item tag">www.bilibili.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/%E5%85%B3%E4%BA%8E/"><span class="level-start"><span class="level-item">关于</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"><span class="level-start"><span class="level-item">计算机</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"><span class="level-start"><span class="level-item">人工智能</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-27T14:15:13.000Z">2022-05-27</time></p><p class="title"><a href="/posts/483895fb/">VGG论文精读</a></p><p class="categories"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/">计算机</a> / <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-09-22T19:21:43.000Z">2021-09-23</time></p><p class="title"><a href="/posts/12577bd1/">AlexNet论文精读</a></p><p class="categories"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/">计算机</a> / <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-09-20T15:27:44.000Z">2021-09-20</time></p><p class="title"><a href="/posts/1/">关于个人博客网站的说明</a></p><p class="categories"><a href="/categories/%E5%85%B3%E4%BA%8E/">关于</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">五月 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">九月 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/AlexNet/"><span class="tag">AlexNet</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VGG/"><span class="tag">VGG</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"><span class="tag">个人博客</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="tag">深度学习</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><span class="tag">神经网络</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BA%E6%96%87/"><span class="tag">论文</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%B4%E6%98%8E/"><span class="tag">说明</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="WilliamLeung的个人博客" height="28"></a><p class="is-size-7"><span>&copy; 2022 William</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdnjs.loli.net/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>